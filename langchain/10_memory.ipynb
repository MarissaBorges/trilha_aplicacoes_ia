{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c5e13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d7fc40",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06a1c63e",
   "metadata": {},
   "source": [
    "A maioria das aplicações de Modelos de LLM possui uma interface conversacional. Um componente essencial de uma conversa é a capacidade de se referir a informações introduzidas anteriormente na conversa. No mínimo, um sistema conversacional deve ser capaz de acessar diretamente alguma janela de mensagens passadas. Um sistema mais complexo precisará ter um modelo de mundo que está constantemente atualizando, o que lhe permite fazer coisas como manter informações sobre entidades e suas relações.\n",
    "\n",
    "Chamamos essa capacidade de armazenar informações sobre interações passadas de \"Memory\", ou memória. LangChain oferece muitas utilidades para adicionar memória a um sistema. Essas utilidades podem ser usadas por si só ou incorporadas de maneira integrada em uma chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a48c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "memory = InMemoryChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d94880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Olá modelo', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Olá user', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.add_user_message('Olá modelo')\n",
    "memory.add_ai_message('Olá user')\n",
    "memory.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd291f1d",
   "metadata": {},
   "source": [
    "## Criando uma conversa com memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c480bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate as cpt\n",
    "\n",
    "prompt = cpt.from_messages([\n",
    "    ('system', 'Você é um tutor de programação chamado Eduardo. Responda as perguntas de forma didática.'),\n",
    "    ('placeholder', '{history}'),\n",
    "    ('human', '{pergunta}')\n",
    "])\n",
    "chain = prompt | ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99cdf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_com_memoria = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key='pergunta',\n",
    "    history_messages_key='history'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d71f620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oi, Thomas! Entendi que você está se apresentando. Se precisar de ajuda com algo específico ou se tiver alguma pergunta, fique à vontade para compartilhar! Estou aqui para ajudar.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'configurable': {'session_id': 'usuario_a'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta': 'O meu nome é Thomas'}, config=config)\n",
    "resposta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99691be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seu nome é Thomas! Se precisar de ajuda ou tiver perguntas, é só me avisar!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = chain_com_memoria.invoke({'pergunta': 'Qual é o meu nome?'}, config=config)\n",
    "resposta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e8fa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, mas eu não tenho acesso a informações pessoais, como o seu nome. Posso te ajudar com perguntas sobre programação ou qualquer outro assunto que você precise! O que você gostaria de saber?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'configurable': {'session_id': 'usuario_b'}}\n",
    "resposta = chain_com_memoria.invoke({'pergunta': 'Qual é o meu nome?'}, config=config)\n",
    "resposta.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
