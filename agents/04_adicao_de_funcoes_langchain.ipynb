{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionando funções externas utilizando LangChain\n",
    "\n",
    "LangChain é um framework para criação de aplicações de IA e naturalmente ele possui ferramentas para facilitar a criação de funções externas para serem passadas a api da OpenAI. Para a utilização do LangChain, será necessário entendermos brevemente uma outra biblioteca de Python chamada pydantic, uma bilioteca para validação de dados que facilita a construção de estruturas de dados mais robustas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduzindo pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como criamos uma estrutura nova sem pydantic\n",
    "\n",
    "Sem utilizar pydantic, podemos criar uma estrutura de dados utilizando funções da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pessoa:\n",
    "    def __init__(self, nome: str, idade: int, peso: float):\n",
    "        self.nome = nome\n",
    "        self.idade = idade\n",
    "        self.peso = peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, criamos uma função que representa uma pessoa e tem os seguintes atributos: nome, idade e peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thomas = Pessoa('Thomas', 18, 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thomas'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thomas.nome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos setar os atributos com qualquer tipo de dado, pois o Python por padrão não faz checagem de tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Pessoa at 0x1f8232d5190>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thomas = Pessoa('Thomas', 18, '52')\n",
    "thomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'52'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thomas.peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como criamos uma estrutura nova usando pydantic\n",
    "\n",
    "A sintaxe de pydantic acaba sendo bem mais simples para a criação de classes de dados, ao compararmos com a criação de classes comuns de Python. Nela, temos que cuidar com a definição do tipo de cada atributo, pois eles serão utilizados para validar se os dados fornecidos estão corretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class pyPessoa(BaseModel):\n",
    "    nome: str\n",
    "    idade: int\n",
    "    peso: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyPessoa(nome='Adriano', idade=32, peso=68.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adriano = pyPessoa(nome='Adriano', idade=32, peso=68)\n",
    "adriano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adriano'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adriano.nome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O interessante é vermos que pydantic fornece uma validação automática de dados. Isso garante uma integridade muito maior em aplicações mais complexas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for pyPessoa\npeso\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='jhflj', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m adriano = \u001b[43mpyPessoa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnome\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdriano\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midade\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeso\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjhflj\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Trabalho\\Programacao\\trilha_aplicacoes_ia\\venv311\\Lib\\site-packages\\pydantic\\main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for pyPessoa\npeso\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='jhflj', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing"
     ]
    }
   ],
   "source": [
    "adriano = pyPessoa(nome='Adriano', idade=32, peso='jhflj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adriano = pyPessoa(nome='Adriano', idade=32, peso='68')\n",
    "adriano.peso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer um nesting de classes de pydantic, onde uma classe de dados recebe como input outra classe de pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyAsimovTeam(funcionarios=[pyPessoa(nome='Adriano', idade=32, peso=68.0)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "class pyAsimovTeam(BaseModel):\n",
    "    funcionarios: List[pyPessoa]\n",
    "\n",
    "pyAsimovTeam(funcionarios= [pyPessoa(nome='Adriano', idade=32, peso=68)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a validação continua fucnionando mesmo com esta estrutura de nesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for pyAsimovTeam\nfuncionarios.0\n  Input should be a valid dictionary or instance of pyPessoa [type=model_type, input_value=<__main__.Pessoa object at 0x000001F823C45E50>, input_type=Pessoa]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpyAsimovTeam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuncionarios\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPessoa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnome\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdriano\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midade\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeso\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m68\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Documentos\\Trabalho\\Programacao\\trilha_aplicacoes_ia\\venv311\\Lib\\site-packages\\pydantic\\main.py:214\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    213\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    216\u001b[39m     warnings.warn(\n\u001b[32m    217\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    219\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    220\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    221\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for pyAsimovTeam\nfuncionarios.0\n  Input should be a valid dictionary or instance of pyPessoa [type=model_type, input_value=<__main__.Pessoa object at 0x000001F823C45E50>, input_type=Pessoa]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type"
     ]
    }
   ],
   "source": [
    "pyAsimovTeam(funcionarios= [Pessoa(nome='Adriano', idade=32, peso=68)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando pydantic para criação de tools da OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )\n",
    "    \n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class UnidadeEnum(str, Enum):\n",
    "    celsius= 'celsius'\n",
    "    fahrenheit= 'fahrenheit'\n",
    "\n",
    "class ObterTemperaturaAtual(BaseModel):\n",
    "    '''Obtem a temperatura atual de uma determinada localidade'''\n",
    "    local: str= Field(description='O nome da cidade', examples=['São Paulo', 'Porto Alegre'])\n",
    "    unidade: Optional[UnidadeEnum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maris\\AppData\\Local\\Temp\\ipykernel_11220\\3747914274.py:3: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  tool_temperatura = convert_pydantic_to_openai_function(ObterTemperaturaAtual)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'ObterTemperaturaAtual',\n",
       " 'description': 'Obtem a temperatura atual de uma determinada localidade',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'local': {'description': 'O nome da cidade',\n",
       "    'examples': ['São Paulo', 'Porto Alegre'],\n",
       "    'type': 'string'},\n",
       "   'unidade': {'description': 'An enumeration.',\n",
       "    'enum': ['celsius', 'fahrenheit'],\n",
       "    'type': 'string'}},\n",
       "  'required': ['local']}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "tool_temperatura = convert_pydantic_to_openai_function(ObterTemperaturaAtual)\n",
    "tool_temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando função externa utilizando LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já sabemos criar funções que os modelos de llm entendam, podemos passar essas funções para os modelos de linguagem através da biblioteca langchain. Para isso temos duas formas, podemos utilizar o parâmetro functions ao chamar o método invoke dos chat_models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"Porto Alegre\",\"unidade\":\"celsius\"}', 'name': 'ObterTemperaturaAtual'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 93, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'function_call', 'logprobs': None}, id='run-bea9f439-138b-479c-9cde-a55c76d14264-0', usage_metadata={'input_tokens': 93, 'output_tokens': 25, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "resposta = chat.invoke('Qual é a temperatura de Porto Alegre', functions=[tool_temperatura])\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou podemos dar um bind e criar um novo componente de chat_model que terá acesso a função sempre que for chamado o invoke. Nestes dois casos, o modelo se comportará com o parâmetro \"auto\" de chamamento de função, ou seja, ele chamará a função quando necessitar, caso contrário se comportará como um modelo de linguagem normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Desculpe, mas não consigo fornecer informações em tempo real, como a temperatura atual de Porto Alegre. Recomendo verificar um site de meteorologia ou um aplicativo de clima para obter as informações mais atualizadas.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 14, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c0ff603a-f912-44d7-87c5-b45ceecb2847-0', usage_metadata={'input_tokens': 14, 'output_tokens': 42, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(model='gpt-4o-mini')\n",
    "chat_com_func = chat.bind(functions=[tool_temperatura])\n",
    "resposta = chat.invoke('Qual é a temperatura de Porto Alegre')\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obrigar o modelo a sempre chamar uma função da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"Porto Alegre\"}', 'name': 'ObterTemperaturaAtual'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 105, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1cc5ffa-2688-4c85-8699-e63d0be8dc40-0', usage_metadata={'input_tokens': 105, 'output_tokens': 7, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = chat.invoke(\n",
    "    'Qual é a temperatura de Porto Alegre',\n",
    "    functions=[tool_temperatura],\n",
    "    function_call={'name': 'ObterTemperaturaAtual'}\n",
    ")\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"São Paulo\"}', 'name': 'ObterTemperaturaAtual'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 99, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a6ac6a7-df0b-4b34-9981-42a5753c7e92-0', usage_metadata={'input_tokens': 99, 'output_tokens': 6, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = chat.invoke(\n",
    "    'Olá',\n",
    "    functions=[tool_temperatura],\n",
    "    function_call={'name': 'ObterTemperaturaAtual'}\n",
    ")\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adicionando a uma chain\n",
    "\n",
    "Podemos adicionar agora este modelo com funções a um prompt e criar uma chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate as cpt\n",
    "\n",
    "prompt = cpt.from_messages([\n",
    "    ('system', 'Você é um assistente amigável chamado Isac'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "chain = prompt | chat.bind(functions=[tool_temperatura])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Olá! Como posso ajudar você hoje?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 98, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4d394013-9f85-4419-bbb7-b295015bfe3b-0', usage_metadata={'input_tokens': 98, 'output_tokens': 9, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Olá'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"local\":\"Florianópolis\"}', 'name': 'ObterTemperaturaAtual'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 104, 'total_tokens': 123, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'function_call', 'logprobs': None}, id='run-296fe475-21c0-47e4-a3ff-05954165f2f3-0', usage_metadata={'input_tokens': 104, 'output_tokens': 19, 'total_tokens': 123, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': 'Qual a temperatura em Floripa?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
